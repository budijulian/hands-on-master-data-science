{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "003_text_mining_preprocessing_and_sentiment_analysis",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/python-for-data-analytic/data-science-in-economics/blob/master/003_text_mining_preprocessing_and_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awnEsM81kACc",
        "colab_type": "text"
      },
      "source": [
        "# Text Mining - Preprocessing and Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47DFpHYnj1-s",
        "colab_type": "text"
      },
      "source": [
        "Text mining is a process of exploring sizeable textual data and find patterns. Text Mining process the text itself. Finding frequency counts of words, length of the sentence, presence/absence of specific words is known as text mining.\n",
        "\n",
        "Natural language processing is one of the components of text mining. NLP helps identified sentiment, finding entities in the sentence, and category of blog/article. Text mining is preprocessed data for text analytics. In Text Analytics, statistical and machine learning algorithm used to classify information.\n",
        "\n",
        "In this section, we will use demonetization tweets data as our text mining case study."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g9GloB_kz_d",
        "colab_type": "text"
      },
      "source": [
        "The demonetization of ₹500 and ₹1000 banknotes was a step taken by the Government of India on 8 November 2016, ceasing the usage of all ₹500 and ₹1000 banknotes of the Mahatma Gandhi Series as a form of legal tender in India from 9 November 2016.\n",
        "\n",
        "The announcement was made by the Prime Minister of India Narendra Modi in an unscheduled live televised address to the nation at 20:15 Indian Standard Time (IST) the same day. In the announcement, Modi declared circulation of all ₹500 and ₹1000 banknotes of the Mahatma Gandhi Series as invalid and announced the issuance of new ₹500 and ₹2000 banknotes of the Mahatma Gandhi New Series in exchange for the old banknotes. \n",
        "\n",
        "The data contains 6000 most recent tweets on #demonetization. There are 6000 rows(one for each tweet) and 14 columns.\n",
        "\n",
        "\n",
        "1.   Text (Tweets)\n",
        "2.   favorited\n",
        "3.   favoriteCount\n",
        "4.   replyToSN\n",
        "5.   created\n",
        "6.   truncated\n",
        "7.   replyToSID\n",
        "8.   id\n",
        "9.   replyToUID\n",
        "10.   statusSource\n",
        "11.   screenName\n",
        "12.   retweetCount\n",
        "13.   isRetweet\n",
        "14.   retweeted\n",
        "\n",
        "Source: https://www.kaggle.com/arathee2/demonetization-in-india-twitter-data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4gYhrbtC4rl",
        "colab_type": "text"
      },
      "source": [
        "***Import Library***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umR5WapdlW_C",
        "colab_type": "text"
      },
      "source": [
        "We need to import some libraries first. Here are the libraries we need to import."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4rKM6x-C97u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import library for Text Analytics\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhqJUzVl_DDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries for Data Manipulation\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI9l6zjVDFVt",
        "colab_type": "text"
      },
      "source": [
        "***Import Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64Md_yy4lwh-",
        "colab_type": "text"
      },
      "source": [
        "Then, import our demonetization tweets dataset into this notebook using Pandas library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duG0Aw--96b6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Data\n",
        "tweets=pd.read_csv('https://raw.githubusercontent.com/dhitology/temporary/master/tweet.csv',encoding='ISO-8859-1')\n",
        "tweets.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2afMRC54AJkM",
        "colab_type": "text"
      },
      "source": [
        "## **Text Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i14aOA1Tm0x0",
        "colab_type": "text"
      },
      "source": [
        "Text preprocessing is traditionally an important step for natural language processing (NLP) tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfbBE4wVm2vC",
        "colab_type": "text"
      },
      "source": [
        "***Select Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMYa8Kw4t4kf",
        "colab_type": "text"
      },
      "source": [
        "We will use only text(tweets) data in this text mining modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUS3DkJj_dsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select Only Text Column\n",
        "text = tweets[['text']]\n",
        "text.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mydcGr66m8tp",
        "colab_type": "text"
      },
      "source": [
        "***Clean the Dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRlihIkS_5I7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Text Cleaning Function using Regex\n",
        "import re\n",
        "\n",
        "def  clean_text(df, text_field):\n",
        "    df[text_field] = df[text_field].str.lower()\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
        "    # remove numbers\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
        "    return df\n",
        "\n",
        "# Apply to the data\n",
        "text_clean = clean_text(text, 'text')\n",
        "text_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gclqDLHICQ0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Stopword\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Apply Stopword to the dataframe\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "text_clean['nostopword'] = text_clean['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "text_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xar0blD5mXsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Punkt\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "text_clean['tokenize'] = text_clean['nostopword'].apply(lambda x: word_tokenize(x))\n",
        "text_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBF-5Jxsm77r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Stemmer\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Create Stemmer Function\n",
        "def word_stemmer(text):\n",
        "    stem_text = [PorterStemmer().stem(i) for i in text]\n",
        "    return stem_text\n",
        "\n",
        "# Apply to the dataframe\n",
        "text_clean['stemming'] = text_clean['tokenize'].apply(lambda x: word_stemmer(x))\n",
        "text_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YnfZR9EnnsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Wordnet Library\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create Lematization Funtion\n",
        "def word_lemmatizer(text):\n",
        "    lem_text = [WordNetLemmatizer().lemmatize(i, pos='v') for i in text]\n",
        "    return lem_text\n",
        "\n",
        "# Apply to a dataframe\n",
        "text_clean['lemmatization'] = text_clean['tokenize'].apply(lambda x: word_lemmatizer(x))\n",
        "text_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-ka-0qPowMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to a New Dataframe\n",
        "text_preprocessed = text_clean['lemmatization'].str.join(\",\") \n",
        "text_preprocessed = text_preprocessed.str.replace(',', ' ', regex=False)\n",
        "text_preprocessed = pd.DataFrame(text_preprocessed)\n",
        "text_preprocessed.rename(columns={'lemmatization': 'text'}, inplace = True)\n",
        "text_preprocessed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNDdgELqtgt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save as CSV\n",
        "text_preprocessed.to_csv('text_preprocessed.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_K6wk7YAap_",
        "colab_type": "text"
      },
      "source": [
        "## **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuSavPiKkSQV",
        "colab_type": "text"
      },
      "source": [
        "Text classification is the process of assigning tags or categories to text according to its content. It’s one of the fundamental tasks in Natural Language Processing (NLP) with broad applications such as sentiment analysis, topic labeling, spam detection, and intent detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4D8U5jI-w1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Module\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
        "\n",
        "# Sentiment Analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "listy = [] \n",
        "for index, row in text_preprocessed.iterrows():\n",
        "  ss = sid.polarity_scores(row['text'])\n",
        "  listy.append(ss)\n",
        "  \n",
        "se = pd.Series(listy)\n",
        "text_preprocessed['polarity'] = se.values\n",
        "display(text_preprocessed.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stJP2xoR_PM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pie Chart Visualization\n",
        "labels = ['negative', 'neutral', 'positive']\n",
        "sizes  = [ss['neg'], ss['neu'], ss['pos']]\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
        "plt.axis('equal') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}